# Assignment 2: N-gram Language Model

## Team 8
- Sabiha Tahsin Soha
- Halleluyah Brhanemesqel
- Harshith Ravi Kopparam

We pair programmed throughout this assignment, with all team members contributing equally to the code, debugging, and writeup.

## Overview

Implementation of an N-gram language model for English, trained on the Europarl corpus. The model can:
- Estimate the probability of a word given its context: P(word | context)
- Compute the probability of entire sentences
- Generate new sentences by sampling from the learned distribution

## Corpus

**Dataset**: Europarl English monolingual corpus (English side of Spanish-English parallel data)
- **Source**: https://www.statmt.org/europarl/v7/
- **Sentences**: 50,000 (configurable)
- **Register/Dialect**: Formal European Parliament proceedings in English
- **Domain**: Political discourse, legislative discussions, formal speeches

## Quick Start

```bash
./run_pipeline.sh
```

This runs the complete pipeline:
1. **preprocessing.py** - Loads Europarl English data and builds vocabulary
2. **training.py** - Trains the N-gram model and saves to disk
3. **evaluate.py** - Computes perplexity on held-out test data
4. **generate.py** - Generates sample sentences from the model

## Requirements

- Python 3.7+
- No external dependencies (uses only standard library)
- Europarl data (shared from Assignment 1)

## Model Details

### Tokenization
- Lowercase all text
- Separate punctuation marks from words
- Split on whitespace
- Replace rare words (appearing < 2 times) with `<UNK>`

### Special Tokens
- `<START>` - Beginning of sentence marker
- `<END>` - End of sentence marker  
- `<UNK>` - Unknown/rare word marker

### N-gram Order
Default: **4-gram** (n=4), as specified by the professor

For a 4-gram model, we estimate:
```
P(w4 | w1, w2, w3) = count(w1, w2, w3, w4) / count(w1, w2, w3)
```

### Smoothing: Stupid Backoff

When we encounter unseen n-grams, we back off to lower-order models with a discount factor α = 0.4:

```
P(w3 | w1, w2) = 
    if count(w1, w2, w3) > 0:
        count(w1, w2, w3) / count(w1, w2)
    else:
        0.4 * P(w3 | w2)  # Back off to bigram
```

This recursively backs off: trigram → bigram → unigram → uniform

### Sentence Probability

```
P(sentence) = ∏ P(wi | wi-2, wi-1)
```

We use log probabilities to avoid numerical underflow:
```
log P(sentence) = Σ log P(wi | wi-2, wi-1)
```

### Perplexity

Perplexity measures how well the model predicts the test data:
```
Perplexity = exp(-1/N * Σ log P(wi | context))
```

Lower perplexity = better model.

## Configuration

Edit `config.py` to change parameters:

```python
NGRAM_ORDER = 4              # 4-gram model (can try 3 or 5)
NUM_TRAINING_SENTENCES = 50000
UNK_THRESHOLD = 2            # Words appearing < this become <UNK>
BACKOFF_ALPHA = 0.4          # Stupid Backoff discount factor
```

## Output Files

```
models/
├── preprocessed_data.pkl    # Tokenized sentences and vocabulary
├── ngram_model.pkl          # Trained model (probability tables)
└── test_sentences.pkl       # Held-out test sentences

output/
├── generated_samples.txt    # Sample generated sentences
└── perplexity_results.txt   # Perplexity evaluation results
```

## Sample Generated Sentences

Here are some sentences generated by sampling from our trained 4-gram model:

1. i do not usually speak too long .
2. this is not a european army .
3. ( applause )
4. in my opinion , a slightly short-sighted view .
5. with regard to eurodac .
6. one of the member states a transitional period of six years .
7. that is why , in our own country , greece , with its never-ending trail of destruction in its wake .
8. for that reason that a careful approach towards the control of isa is needed .
9. this is a crucial task , one which is too detailed , too extensive and need to be drawn up in collaboration with the council and parliament to approach it
10. you cannot abolish risk because it is <UNK> behaviour which can include physical attacks on individuals .

**Observations**: 
- The 4-gram model produces more coherent sentences than a trigram would
- Shorter sentences are often grammatically correct and natural-sounding
- Longer sentences capture parliamentary speech patterns well but can drift
- The model has learned domain-specific language ("member states", "transitional period", "eurodac")
- Common phrases like "(applause)" and "mr president" appear naturally

## Results

### Corpus Statistics
- **Sentences**: 49,945
- **Total tokens**: 1,396,479
- **Vocabulary size**: 14,617 (after UNK threshold of 2)
- **Average sentence length**: 28.0 words
- **UNK tokens**: 0.61% of all tokens

### N-gram Statistics (4-gram Model)
- **Unigram contexts**: 1
- **Bigram contexts**: 14,411 unique
- **Trigram contexts**: 255,274 unique
- **4-gram contexts**: 671,649 unique

### Perplexity
- **Training set** (sample of 1,000): 2.93
- **Test set** (4,995 sentences): 181.78

The very low training perplexity (2.93) shows that the 4-gram model memorizes many exact sequences from training. The higher test perplexity (181.78) compared to a trigram model is expected because:
1. 4-grams are sparser - many test contexts were never seen in training
2. The model backs off more frequently to lower-order n-grams
3. This is a known trade-off: higher-order models can be more precise but suffer more from data sparsity

### Real vs Shuffled Comparison

The model clearly distinguishes properly ordered sentences from shuffled ones:

| Sentence Type | Example | Log Probability |
|--------------|---------|-----------------|
| Original | "we really cannot tolerate behaviour of this kind !" | -61.26 |
| Shuffled | "tolerate we ! kind this cannot really of behaviour" | -90.58 |
| **Difference** | | **29.3 log-units better** |

| Original | "they relate to the different areas of italy ." | -42.14 |
| Shuffled | ". to italy the different they areas of relate" | -82.86 |
| **Difference** | | **40.7 log-units better** |

This demonstrates that the model has learned meaningful word order patterns from the training data.

## Running Individual Steps

```bash
python3 preprocessing.py       # Step 1: Preprocess data
python3 training.py            # Step 2: Train model
python3 evaluate.py            # Step 3: Evaluate model
python3 generate.py            # Step 4: Generate sentences
```

## Project Structure

```
Assignment 2/
├── config.py              # Configuration settings
├── preprocessing.py       # Data loading and tokenization
├── ngram_model.py         # NGramModel class implementation
├── training.py            # Model training script
├── generate.py            # Sentence generation script
├── evaluate.py            # Perplexity evaluation script
├── run_pipeline.sh        # Run all steps
└── README.md              # This file
```

## Implementation Notes

### Key Design Decisions

1. **Data Structure**: Following Professor Rudnick's approach, we use `defaultdict(Counter)` to store n-gram counts efficiently:
   ```python
   counts[context][word] = count
   ```

2. **Sliding Window**: We pad sentences with `<START>` tokens and slide a window across to collect all n-grams.

3. **Multi-order Storage**: We store counts and probabilities for all orders (unigram through n-gram) to support backoff smoothing.

4. **Vocabulary Handling**: Rare words are replaced with `<UNK>` during preprocessing, not at inference time, ensuring consistent treatment.

### Potential Extensions

- **Interpolation Smoothing**: Use weighted combination instead of backoff
- **Kneser-Ney Smoothing**: More sophisticated smoothing for better handling of rare events
- **4-gram or 5-gram models**: With sufficient data, higher-order models may perform better
- **Perplexity vs N analysis**: Compare perplexity for different N values
